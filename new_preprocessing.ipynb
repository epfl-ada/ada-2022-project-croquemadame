{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00386e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import used libraries\n",
    "import pandas as pd                        # pandas for data analysis\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import matplotlib.pyplot as plt            # matplotlib for data visualisation\n",
    "import json\n",
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26e78739",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PATHS ###\n",
    "\n",
    "DIR = \"../data/\"\n",
    "\n",
    "#read\n",
    "TIMESERIES_PATH = DIR + \"df_timeseries_en.tsv.gz\"\n",
    "CHANNELS_PATH   = DIR + \"df_channels_en.tsv.gz\"\n",
    "\n",
    "#write\n",
    "ENT_TIMESERIES_PATH = DIR + \"ent_timeseries_en.tsv.zip\"\n",
    "ENT_CHANNELS_PATH   = DIR + \"ent_channels_en.tsv.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac21de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "\n",
    "timeseries = pd.read_csv(TIMESERIES_PATH, sep='\\t')\n",
    "channels   = pd.read_csv(CHANNELS_PATH, sep='\\t')\n",
    "\n",
    "timeseries['datetime'] = pd.to_datetime(timeseries['datetime'])\n",
    "channels['join_date'] = pd.to_datetime(channels['join_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e79ba693",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TREATMENT ###\n",
    "\n",
    "#compute the number of subs at the start of the period for each youtuber \n",
    "channels = pd.merge(channels,\n",
    "                    timeseries.sort_values(by='datetime').drop_duplicates('channel')[['channel','subs']].rename(columns={\"subs\":\"initial_subs\"}),\n",
    "                    on = 'channel')\n",
    "\n",
    "#keep only channels that started between 5k and 10k from the entertaining category\n",
    "ent_channels = channels[(channels['initial_subs'] > 5e3) &\n",
    "                        (channels['initial_subs'] < 10e3) &\n",
    "                        (channels['category_cc'] == 'Entertainment')]\n",
    "\n",
    "ent_timeseries = pd.merge(timeseries, ent_channels['channel'], on='channel').drop(['category'], axis=1)\n",
    "\n",
    "#compute the weekly evolution for each channel in timeseries\n",
    "ent_timeseries['evolution'] = ent_timeseries['delta_subs']/ent_timeseries['subs']\n",
    "\n",
    "#compute the evolution score by taking the mean weekly evolution\n",
    "evo_score = ent_timeseries.groupby('channel').mean()['evolution'].rename('evo_score')\n",
    "ent_channels = pd.merge(ent_channels, evo_score, on='channel')\n",
    "\n",
    "# #keeps only 25% top and 25% bottom channels\n",
    "top_channels = ent_channels.nlargest(int(len(ent_channels)*0.25), 'evo_score')[['channel','evo_score']]\n",
    "bottom_channels = ent_channels.nsmallest(int(len(ent_channels)*0.25), 'evo_score')[['channel','evo_score']]\n",
    "evo_channels = pd.concat([top_channels, bottom_channels]).sort_values('evo_score', ascending=False)\n",
    "evo_channels['has_buzzed'] = 0 + 1 * (evo_channels['evo_score'] > bottom_channels['evo_score'].max())\n",
    "\n",
    "ent_channels = pd.merge(ent_channels, evo_channels.drop(['evo_score'], axis=1), on='channel')\n",
    "ent_timeseries = pd.merge(ent_timeseries, ent_channels[['channel', 'has_buzzed']], on='channel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c037a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPORTS ###\n",
    "\n",
    "ent_channels.to_csv(ENT_CHANNELS_PATH, index=False, compression={'method':'zip'})\n",
    "ent_timeseries.to_csv(ENT_TIMESERIES_PATH, index=False, compression={'method':'zip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40da98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "############################################ METADATA ############################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bdcbf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################## PATH ##############################################\n",
    "\n",
    "METADATA_PATH   = DIR + \"_raw_yt_metadata.jsonl.zst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ec28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "############################## READ AND SPLIT INTO SMALLER CSV FILES #############################\n",
    "\n",
    "class zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        self.fh = open(file, 'rb')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstd.ZstdDecompressor()\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def readlines(self):\n",
    "        while True:\n",
    "            chunk = self.reader.read(self.chunk_size).decode(\"utf-8\", errors=\"ignore\")\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]\n",
    "\n",
    "reader = zreader(METADATA_PATH)\n",
    "metadata = []\n",
    "df_metadata = pd.DataFrame([])\n",
    "\n",
    "idx = 0\n",
    "store_idx = 0\n",
    "save_idx = 0\n",
    "\n",
    "min_upload_date, max_upload_date = pd.to_datetime(\"01-01-2015\"), pd.to_datetime(\"09-30-2019\")\n",
    "for line in reader.readlines():\n",
    "    line_dict = json.loads(line)\n",
    "    \n",
    "    if (pd.to_datetime(line_dict[\"upload_date\"]) < max_upload_date) & \\\n",
    "       (pd.to_datetime(line_dict[\"upload_date\"]) > min_upload_date) :\n",
    "        \n",
    "        \n",
    "        del line_dict['description']\n",
    "        del line_dict['crawl_date']\n",
    "        del line_dict['categories']\n",
    "        \n",
    "        metadata.append(line_dict)\n",
    "    idx += 1\n",
    "    if idx%100000 == 0:\n",
    "        print(idx)\n",
    "    \n",
    "    #store in a dataframe every 1 million\n",
    "    if len(metadata) >= 1000000:\n",
    "        if store_idx < 9 : print(\" - STORE\", store_idx)\n",
    "        df_metadata = pd.concat([df_metadata, pd.DataFrame(metadata)])\n",
    "        metadata = []\n",
    "        store_idx += 1\n",
    "        \n",
    "        \n",
    "    #save dataframe every 10 million\n",
    "    if len(df_metadata) >= 10000000:\n",
    "        print(\" - SAVE \", save_idx)\n",
    "        \n",
    "        S_METADATA_PATH = DIR + \"metadata/_raw_yt_metadata\" + str(save_idx) + \".tsv.zip\"\n",
    "    \n",
    "        df_metadata.to_csv(S_METADATA_PATH, index=False, compression={'method':'zip'})\n",
    "        df_metadata = pd.DataFrame([])\n",
    "        store_idx = 0\n",
    "        save_idx += 1\n",
    "        \n",
    "print(\" - SAVE \", save_idx)\n",
    "        \n",
    "S_METADATA_PATH = DIR + \"metadata/_raw_yt_metadata\" + str(save_idx) + \".tsv.zip\"\n",
    "\n",
    "df_metadata.to_csv(S_METADATA_PATH, index=False, compression={'method':'zip'})\n",
    "df_metadata = pd.DataFrame([])\n",
    "store_idx = 0\n",
    "save_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6c05b8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start metadata  6\n",
      "Path done - \n",
      "Read done - \n",
      "Treatment done - \n",
      "Write done -\n",
      "Done metadata  6\n"
     ]
    }
   ],
   "source": [
    "# TREAT THE 6 METADATA\n",
    "for i in range(7):\n",
    "    print(\"Start metadata \", i)\n",
    "    \n",
    "    # PATH\n",
    "    METADATA_PATH = DIR + \"metadata/_raw_yt_metadata\" + str(i) + \".tsv.zip\"\n",
    "    ENT_METADATA_PATH = DIR + \"metadata/ent_metadata\" + str(i) + \".tsv.zip\"\n",
    "    print(\"Path done - \")\n",
    "    \n",
    "    # READ\n",
    "    metadata = pd.read_csv(METADATA_PATH)\n",
    "    print(\"Read done - \")\n",
    "    \n",
    "    # TREATMENT\n",
    "    metadata = metadata.rename(columns={'channel_id':'channel'})\n",
    "    ent_metadata = pd.merge(metadata, ent_channels['channel'])\n",
    "    print(\"Treatment done - \")\n",
    "    \n",
    "    # WRITE\n",
    "    ent_metadata.to_csv(ENT_METADATA_PATH, index=False, compression={'method':'zip'})\n",
    "    print(\"Write done -\")\n",
    "    \n",
    "    print(\"Done metadata \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbd77bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start metadata  0\n",
      "Path done - \n",
      "Read done - \n",
      "Concat done - \n",
      "Start metadata  1\n",
      "Path done - \n",
      "Read done - \n",
      "Concat done - \n",
      "Start metadata  2\n",
      "Path done - \n",
      "Read done - \n",
      "Concat done - \n",
      "Start metadata  3\n",
      "Path done - \n",
      "Read done - \n",
      "Concat done - \n",
      "Start metadata  4\n",
      "Path done - \n",
      "Read done - \n",
      "Concat done - \n",
      "Start metadata  5\n",
      "Path done - \n",
      "Read done - \n",
      "Concat done - \n",
      "Start metadata  6\n",
      "Path done - \n",
      "Read done - \n",
      "Concat done - \n",
      "Write done -\n"
     ]
    }
   ],
   "source": [
    "# STORE ALL DATAFRAMES IN ONLY ONE\n",
    "ent_metadata = pd.DataFrame()\n",
    "for i in range(7):\n",
    "    print(\"Start metadata \", i)\n",
    "    \n",
    "    # PATH\n",
    "    METADATA_PATH = DIR + \"metadata/ent_metadata\" + str(i) + \".tsv.zip\"\n",
    "    print(\"Path done - \")\n",
    "    \n",
    "    # READ\n",
    "    metadata = pd.read_csv(METADATA_PATH)\n",
    "    print(\"Read done - \")\n",
    "    \n",
    "    # CONCAT\n",
    "    ent_metadata = pd.concat([ent_metadata, metadata], ignore_index=True)\n",
    "    print(\"Concat done - \")\n",
    "\n",
    "\n",
    "# WRITE\n",
    "ENT_METADATA_PATH = DIR + \"ent_metadata_en.tsv.zip\"\n",
    "ent_metadata.to_csv(ENT_METADATA_PATH, index=False, compression={'method':'zip'})\n",
    "print(\"Write done -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61f65eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP COMMON CHANNELS BETWEEN df_channels, df_metadata AND df_timeseries\n",
    "# df_metadata HAS LESS CHANNELS THAN df_channels AND df_timeseries (SOME CHANNELS WITHOUT ANY VIDEO ?)\n",
    "\n",
    "#dir\n",
    "DIR = \"../data/\"\n",
    "\n",
    "#read path\n",
    "TIMESERIES_PATH = DIR + \"ent_timeseries_en.tsv.zip\"\n",
    "CHANNELS_PATH   = DIR + \"ent_channels_en.tsv.zip\"\n",
    "METADATA_PATH   = DIR + \"ent_metadata_en.tsv.zip\"\n",
    "\n",
    "#imports\n",
    "timeseries = pd.read_csv(TIMESERIES_PATH)\n",
    "channels   = pd.read_csv(CHANNELS_PATH)\n",
    "metadata   = pd.read_csv(METADATA_PATH)\n",
    "\n",
    "#treatment\n",
    "channel_ids = metadata[['channel']].drop_duplicates()\n",
    "ent_channels = pd.merge(channels, channel_ids)\n",
    "ent_timeseries = pd.merge(timeseries, channel_ids)\n",
    "\n",
    "#write path\n",
    "ENT_TIMESERIES_PATH = DIR + \"ent_timeseries_en.tsv.zip\"\n",
    "ENT_CHANNELS_PATH   = DIR + \"ent_channels_en.tsv.zip\"\n",
    "\n",
    "#exports\n",
    "ent_channels.to_csv(ENT_CHANNELS_PATH, index=False, compression={'method':'zip'})\n",
    "ent_timeseries.to_csv(ENT_TIMESERIES_PATH, index=False, compression={'method':'zip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65477aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "18c32a2b66552a3f09b8a3b497862286fc4b79643dbf6273d2fe975b8b827621"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
