{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00386e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import used libraries\n",
    "import pandas as pd                        # pandas for data analysis\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import matplotlib.pyplot as plt            # matplotlib for data visualisation\n",
    "import json\n",
    "import zstandard as zstd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e78739",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PATHS ###\n",
    "\n",
    "DIR = \"data/\"\n",
    "\n",
    "#read\n",
    "TIMESERIES_PATH = DIR + \"df_timeseries_en.tsv.gz\"\n",
    "CHANNELS_PATH   = DIR + \"df_channels_en.tsv.gz\"\n",
    "\n",
    "#write\n",
    "S_TIMESERIES_PATH = DIR + \"s_df_timeseries_en.tsv.zip\"\n",
    "S_CHANNELS_PATH   = DIR + \"s_df_channels_en.tsv.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac21de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS ###\n",
    "\n",
    "timeseries = pd.read_csv(TIMESERIES_PATH, sep='\\t')\n",
    "channels   = pd.read_csv(CHANNELS_PATH, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79ba693",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TREATMENT ###\n",
    "\n",
    "#keep only channels that have gone from 10k to 500k subscribers in the period\n",
    "channels_sub10k  = timeseries[timeseries['subs'] < 10e3]['channel'].drop_duplicates()\n",
    "channels_sub500k = timeseries[timeseries['subs'] > 500e3]['channel'].drop_duplicates()\n",
    "s_channels_ids   = pd.merge(channels_sub500k,channels_sub10k)\n",
    "\n",
    "s_channels   = pd.merge(channels, s_channels_ids)\n",
    "s_timeseries = pd.merge(timeseries, s_channels_ids).drop(['category'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c037a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPORTS ###\n",
    "\n",
    "s_channels.to_csv(S_CHANNELS_PATH, index=False, compression={'method':'zip'})\n",
    "s_timeseries.to_csv(S_TIMESERIES_PATH, index=False, compression={'method':'zip'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT CATEGORIES ###\n",
    "\n",
    "categories = channels.groupby('category_cc').count()[['channel']]\n",
    "categories['success_channel'] = s_channel.groupby('category_cc').count()[['channel']]\n",
    "categories['success_rate'] = categories['success_channel']/categories['channel']\n",
    "\n",
    "plt.bar(categories.index, categories['channel'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Number of channels per categories on youtube')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(categories.index, categories['success_channel'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Number of successful channels per categories on youtube')\n",
    "plt.show()\n",
    "\n",
    "plt.bar(categories.index, categories['success_rate'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Rate of successful channels in each categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "############################################ METADATA ############################################\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdcbf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################################## PATH ##############################################\n",
    "\n",
    "METADATA_PATH   = DIR + \"_raw_yt_metadata.jsonl.zst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ec28a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "############################## READ AND SPLIT INTO SMALLER CSV FILES #############################\n",
    "\n",
    "class zreader:\n",
    "\n",
    "    def __init__(self, file, chunk_size=16384):\n",
    "        self.fh = open(file, 'rb')\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dctx = zstd.ZstdDecompressor()\n",
    "        self.reader = self.dctx.stream_reader(self.fh)\n",
    "        self.buffer = ''\n",
    "\n",
    "    def readlines(self):\n",
    "        while True:\n",
    "            chunk = self.reader.read(self.chunk_size).decode(\"utf-8\", errors=\"ignore\")\n",
    "            if not chunk:\n",
    "                break\n",
    "            lines = (self.buffer + chunk).split(\"\\n\")\n",
    "\n",
    "            for line in lines[:-1]:\n",
    "                yield line\n",
    "\n",
    "            self.buffer = lines[-1]\n",
    "\n",
    "reader = zreader(METADATA_PATH)\n",
    "metadata = []\n",
    "df_metadata = pd.DataFrame([])\n",
    "\n",
    "idx = 0\n",
    "store_idx = 0\n",
    "save_idx = 0\n",
    "\n",
    "min_upload_date, max_upload_date = pd.to_datetime(\"01-01-2015\"), pd.to_datetime(\"09-30-2019\")\n",
    "for line in reader.readlines():\n",
    "    line_dict = json.loads(line)\n",
    "    \n",
    "    if (pd.to_datetime(line_dict[\"upload_date\"]) < max_upload_date) & \\\n",
    "       (pd.to_datetime(line_dict[\"upload_date\"]) > min_upload_date) :\n",
    "        \n",
    "        del line_dict['description']\n",
    "        del line_dict['crawl_date']\n",
    "        del line_dict['categories']\n",
    "        \n",
    "        metadata.append(line_dict)\n",
    "    idx += 1\n",
    "    if idx%100000 == 0:\n",
    "        print(idx)\n",
    "    \n",
    "    #store in a dataframe every 1 million\n",
    "    if len(metadata) >= 1000000:\n",
    "        if store_idx < 9 : print(\" - STORE\", store_idx)\n",
    "        df_metadata = pd.concat([df_metadata, pd.DataFrame(metadata)])\n",
    "        metadata = []\n",
    "        store_idx += 1\n",
    "        \n",
    "        \n",
    "    #save dataframe every 10 million\n",
    "    if len(df_metadata) >= 10000000:\n",
    "        print(\" - SAVE \", save_idx)\n",
    "        \n",
    "        S_METADATA_PATH = DIR + \"metadata/_raw_yt_metadata\" + str(save_idx) + \".tsv.zip\"\n",
    "    \n",
    "        df_metadata.to_csv(S_METADATA_PATH, index=False, compression={'method':'zip'})\n",
    "        df_metadata = pd.DataFrame([])\n",
    "        store_idx = 0\n",
    "        save_idx += 1\n",
    "        \n",
    "print(\" - SAVE \", save_idx)\n",
    "        \n",
    "S_METADATA_PATH = DIR + \"metadata/_raw_yt_metadata\" + str(save_idx) + \".tsv.zip\"\n",
    "\n",
    "df_metadata.to_csv(S_METADATA_PATH, index=False, compression={'method':'zip'})\n",
    "df_metadata = pd.DataFrame([])\n",
    "store_idx = 0\n",
    "save_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c05b8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TREAT THE 6 METADATA\n",
    "for i in range(save_idx):\n",
    "    print(\"Start metadata \", i)\n",
    "    \n",
    "    # PATH\n",
    "    METADATA_PATH = DIR + \"metadata/_raw_yt_metadata\" + str(i) + \".tsv.zip\"\n",
    "    S_METADATA_PATH = DIR + \"metadata/s_metadata\" + str(i) + \".tsv.zip\"\n",
    "    print(\"Path done - \")\n",
    "    \n",
    "    # READ\n",
    "    metadata = pd.read_csv(METADATA_PATH)\n",
    "    print(\"Read done - \")\n",
    "    \n",
    "    # TREATMENT\n",
    "    metadata = metadata.rename(columns={'channel_id':'channel'})\n",
    "    s_metadata = pd.merge(metadata, s_channels_ids)\n",
    "    print(\"Treatment done - \")\n",
    "    \n",
    "    # WRITE\n",
    "    s_metadata.to_csv(S_METADATA_PATH, index=False, compression={'method':'zip'})\n",
    "    print(\"Write done -\")\n",
    "    \n",
    "    print(\"Done metadata \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd77bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STORE ALL DATAFRAMES IN ONLY ONE\n",
    "s_metadata = pd.DataFrame()\n",
    "for i in range(save_idx):\n",
    "    print(\"Start metadata \", i)\n",
    "    \n",
    "    # PATH\n",
    "    METADATA_PATH = DIR + \"metadata/s_metadata\" + str(i) + \".tsv.zip\"\n",
    "    print(\"Path done - \")\n",
    "    \n",
    "    # READ\n",
    "    metadata = pd.read_csv(METADATA_PATH)\n",
    "    print(\"Read done - \")\n",
    "    \n",
    "    # CONCAT\n",
    "    s_metadata = pd.concat([s_metadata, metadata], ignore_index=True)\n",
    "    print(\"Concat done - \")\n",
    "\n",
    "\n",
    "# WRITE\n",
    "S_METADATA_PATH = DIR + \"s_df_metadata_en.tsv.zip\"\n",
    "s_metadata.to_csv(S_METADATA_PATH, index=False, compression={'method':'zip'})\n",
    "print(\"Write done -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f65eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEP COMMON CHANNELS BETWEEN df_channels, df_metadata AND df_timeseries\n",
    "# df_metadata HAS LESS CHANNELS THAN df_channels AND df_timeseries (SOME CHANNELS WITHOUT ANY VIDEO ?)\n",
    "\n",
    "#dir\n",
    "DIR = \"data/\"\n",
    "\n",
    "#read path\n",
    "TIMESERIES_PATH = DIR + \"s_df_timeseries_en.tsv.zip\"\n",
    "CHANNELS_PATH   = DIR + \"s_df_channels_en.tsv.zip\"\n",
    "METADATA_PATH   = DIR + \"s_df_metadata_en.tsv.zip\"\n",
    "\n",
    "#imports\n",
    "timeseries = pd.read_csv(TIMESERIES_PATH)\n",
    "channels   = pd.read_csv(CHANNELS_PATH)\n",
    "metadata   = pd.read_csv(METADATA_PATH)\n",
    "\n",
    "#treatment\n",
    "channel_ids = metadata[['channel']].drop_duplicates()\n",
    "s_channels = pd.merge(channels, channel_ids)\n",
    "s_timeseries = pd.merge(timeseries, channel_ids)\n",
    "\n",
    "#write path\n",
    "S_TIMESERIES_PATH = DIR + \"s_df_timeseries_en.tsv.zip\"\n",
    "S_CHANNELS_PATH   = DIR + \"s_df_channels_en.tsv.zip\"\n",
    "\n",
    "#exports\n",
    "s_channels.to_csv(S_CHANNELS_PATH, index=False, compression={'method':'zip'})\n",
    "s_timeseries.to_csv(S_TIMESERIES_PATH, index=False, compression={'method':'zip'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
